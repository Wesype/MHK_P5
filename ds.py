import asyncio
import json
import os
import re
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from bs4 import BeautifulSoup
from db_simple import DossiersManager
from dotenv import load_dotenv

load_dotenv()

async def login_and_scrape_all():
    # Configuration du proxy r√©sidentiel
    proxy_config = None
    proxy_server = os.getenv('PROXY_SERVER')
    proxy_username = os.getenv('PROXY_USERNAME')
    proxy_password = os.getenv('PROXY_PASSWORD')
    
    if proxy_server and proxy_username and proxy_password:
        proxy_config = f"http://{proxy_username}:{proxy_password}@{proxy_server}"
        print(f"üåê Utilisation du proxy: {proxy_server}")
    
    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        proxy_config=proxy_config
    )
    
    all_dossiers = []
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # √âTAPE 1 : Connexion
        print(" Connexion en cours...")
        
        login_config = CrawlerRunConfig(
            js_code=[
                # Masquer les traces d'automatisation
                """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['fr-FR', 'fr', 'en-US', 'en']});
                """,
                "await new Promise(r => setTimeout(r, 500));",
                """
                document.querySelector('#user_email').value = 'administration.etrangers@mhk-avocats.com';
                document.querySelector('#user_password').value = 'MHKavocats-christiani2025@';
                """,
                "await new Promise(r => setTimeout(r, 500));",
                "document.querySelector('input[type=\"submit\"][value=\"Se connecter\"]').click();",
                "await new Promise(r => setTimeout(r, 5000));"  # Attendre la redirection
            ],
            delay_before_return_html=2.0,
            page_timeout=30000,  # 30 secondes max
            session_id="demarches_session"  # Maintenir la session
        )
        
        result = await crawler.arun(
            url="https://demarche.numerique.gouv.fr/users/sign_in",
            config=login_config
        )
        
        if not result.success:
            print(" √âchec de la connexion")
            return
        
        print("‚úÖ Connexion effectu√©e!\n")
        
        # √âTAPE 2 : D√©finir tous les statuts √† scraper
        statuts_config = [
            {'nom': 'en-cours', 'statut': 'en-cours'},
            {'nom': 'trait√©s', 'statut': 'traites'},
            {'nom': 'dossiers invit√©s', 'statut': 'dossiers-invites'},
            {'nom': 'expirants', 'statut': 'dossiers-expirant'},  # Scraper mais ne pas inclure dans la table principale
            {'nom': 'corbeille', 'statut': 'dossiers-supprimes'}
        ]
        
        async def get_max_pages(crawler, statut, session_id):
            """D√©tecte automatiquement le nombre de pages pour un statut donn√©"""
            url = f"https://demarche.numerique.gouv.fr/dossiers?page=1&statut={statut}"
            
            try:
                config = CrawlerRunConfig(
                    delay_before_return_html=2.0,
                    page_timeout=30000,
                    session_id=session_id
                )
                
                result = await crawler.arun(url=url, config=config)
                
                if not result.success:
                    print(f"‚ö†Ô∏è  Impossible de d√©tecter le nombre de pages pour {statut}, utilisation de 1 par d√©faut")
                    return 1
                
                soup = BeautifulSoup(result.html, 'html.parser')
            except Exception as e:
                print(f"‚ùå Erreur lors de la d√©tection des pages pour {statut}: {e}")
                return 1
            
            # Chercher le lien "Dernier" qui contient le num√©ro de la derni√®re page
            last_link = soup.find('a', class_='fr-pagination__link--last')
            
            if last_link and last_link.get('href'):
                # Extraire le num√©ro de page de l'URL
                match = re.search(r'page=(\d+)', last_link['href'])
                if match:
                    max_pages = int(match.group(1))
                    return max_pages
            
            # Si pas de lien "Dernier", chercher tous les liens de pagination
            pagination_links = soup.find_all('a', class_='fr-pagination__link')
            page_numbers = []
            
            for link in pagination_links:
                if link.get('href'):
                    match = re.search(r'page=(\d+)', link['href'])
                    if match:
                        page_numbers.append(int(match.group(1)))
            
            if page_numbers:
                return max(page_numbers)
            
            # Par d√©faut, retourner 1
            return 1
        
        # √âTAPE 2.5 : D√©tecter automatiquement le nombre de pages pour chaque statut
        print("üîç D√©tection automatique du nombre de pages...\n")
        
        for statut_info in statuts_config:
            max_pages = await get_max_pages(crawler, statut_info['statut'], "demarches_session")
            statut_info['pages'] = max_pages
            print(f"   üìä {statut_info['nom']}: {max_pages} pages")
        
        print()
        
        page_config = CrawlerRunConfig(
            delay_before_return_html=2.0,
            page_timeout=30000,
            session_id="demarches_session"
        )
        
        # Boucle sur chaque statut
        for statut_info in statuts_config:
            statut_nom = statut_info['nom']
            statut_param = statut_info['statut']
            nb_pages = statut_info['pages']
            
            print(f"\n{'='*60}")
            print(f" Scraping des dossiers '{statut_nom}' ({nb_pages} pages)")
            print(f"{'='*60}\n")
            
            for page_num in range(1, nb_pages + 1):
                url = f"https://demarche.numerique.gouv.fr/dossiers?page={page_num}&statut={statut_param}"
                
                print(f" [{statut_nom}] Page {page_num}/{nb_pages}... ", end='', flush=True)
                
                result = await crawler.arun(url=url, config=page_config)
                
                if not result.success:
                    print(f" √âchec")
                    continue
                
                # Parser avec BeautifulSoup
                soup = BeautifulSoup(result.html, 'html.parser')
                dossiers = soup.find_all('div', class_='card')
                
                # Debug: v√©rifier si on est toujours connect√©
                if len(dossiers) == 0:
                    # V√©rifier si on est redirig√© vers la page de login
                    login_form = soup.find('form', action='/users/sign_in')
                    if login_form:
                        print(f"‚ùå Session perdue - redirection vers login")
                    else:
                        # Sauvegarder le HTML pour debug
                        with open(f'debug_page_{statut_param}_{page_num}.html', 'w', encoding='utf-8') as f:
                            f.write(result.html[:5000])  # Premiers 5000 caract√®res
                        print(f"‚ö†Ô∏è  0 dossiers (HTML sauvegard√© pour debug)")
                else:
                    print(f"‚úÖ {len(dossiers)} dossiers")
                
                for dossier in dossiers:
                    try:
                        # ID
                        dossier_id = dossier.get('id', '').replace('dossier_', '')
                        
                        # Titre et lien
                        titre_elem = dossier.find('h3', class_='card-title')
                        titre_link = titre_elem.find('a') if titre_elem else None
                        titre = titre_link.text.strip() if titre_link else ""
                        lien = f"https://demarche.numerique.gouv.fr{titre_link['href']}" if titre_link and titre_link.get('href') else ""
                        
                        # Statut
                        badge = dossier.find('span', class_='fr-badge')
                        statut = badge.text.strip() if badge else ""
                        
                        # Demandeur
                        identite_elem = dossier.find('p', class_='fr-icon-user-line')
                        if identite_elem:
                            demandeur = identite_elem.get_text(strip=True).replace('Identit√© du demandeur', '').strip()
                        else:
                            demandeur = ""
                        
                        # Dates
                        date_elem = dossier.find('p', class_='fr-icon-edit-box-line')
                        date_creation = ""
                        date_modification = ""
                        
                        if date_elem:
                            dates_text = date_elem.get_text()
                            if "Cr√©√© le" in dates_text:
                                date_creation = dates_text.split("Cr√©√© le")[1].split("modifi√©")[0].strip()
                            if "modifi√© le" in dates_text:
                                date_modification = dates_text.split("modifi√© le")[1].strip()
                        
                        # Ajouter le dossier avec la cat√©gorie
                        all_dossiers.append({
                            'numero': dossier_id,
                            'titre': titre,
                            'lien': lien,
                            'categorie': statut_nom,  # Ajout de la cat√©gorie
                            'statut': statut,
                            'demandeur': demandeur,
                            'date_creation': date_creation,
                            'date_modification': date_modification,
                            'page': page_num
                        })
                        
                    except Exception as e:
                        print(f"\n   ‚ö†Ô∏è Erreur: {e}")
                
                # Petit d√©lai entre les pages
                await asyncio.sleep(0.5)
        
        # √âTAPE 3 : Sauvegarder
        print(f"\n Sauvegarde de {len(all_dossiers)} dossiers...")
        
        # JSON
        with open('dossiers_complets.json', 'w', encoding='utf-8') as f:
            json.dump(all_dossiers, f, ensure_ascii=False, indent=2)
        
        # CSV
        import csv
        with open('dossiers_complets.csv', 'w', encoding='utf-8', newline='') as f:
            if all_dossiers:
                writer = csv.DictWriter(f, fieldnames=all_dossiers[0].keys())
                writer.writeheader()
                writer.writerows(all_dossiers)
        
        print(f"\n TERMIN√â!")
        print(f"   {len(all_dossiers)} dossiers extraits")
        print(f"   dossiers_complets.json")
        print(f"   dossiers_complets.csv")
        
        # √âTAPE 4 : Comparer avec PostgreSQL et d√©tecter les changements
        try:
            db = DossiersManager()
            db.connect()
            
            # S√©parer les dossiers expirants des autres
            expirants = [d for d in all_dossiers if d['categorie'] == 'expirants']
            autres_dossiers = [d for d in all_dossiers if d['categorie'] != 'expirants']
            
            print(f"\nüìä R√©partition:")
            print(f"   Dossiers normaux: {len(autres_dossiers)}")
            print(f"   Dossiers expirants (tracking s√©par√©): {len(expirants)}")
            
            # Processus complet: cr√©er table temp, comparer, remplacer
            changements = db.process_scraping(autres_dossiers, expirants)
            
            db.disconnect()
            
            # √âTAPE 5 : Si des changements d√©tect√©s, t√©l√©charger les PDFs et envoyer au webhook
            if changements:
                print(f"\n‚ú® {len(changements)} changements d√©tect√©s")
                print(f"üì• Lancement du t√©l√©chargement des PDFs et envoi au webhook...\n")
                
                # Importer et ex√©cuter le t√©l√©chargement
                from download_pdfs import download_changed_dossiers
                await download_changed_dossiers()
            
        except Exception as e:
            print(f"\n ‚ö†Ô∏è  Erreur PostgreSQL: {e}")
            print("   Les donn√©es sont sauvegard√©es dans les fichiers JSON/CSV")
        
        # Stats
        statuts = {}
        for d in all_dossiers:
            statuts[d['statut']] = statuts.get(d['statut'], 0) + 1
        
        print(f"\n R√©partition par statut:")
        for statut, count in statuts.items():
            print(f"   - {statut}: {count}")
        
        return all_dossiers

if __name__ == "__main__":
    # Ex√©cuter seulement si appel√© directement
    dossiers = asyncio.run(login_and_scrape_all())